# System Architecture

## Overview

The Gaussian Splatting Knowledge Graph is a multi-agent system designed to construct a comprehensive knowledge graph from academic papers. The system follows a modular, scalable architecture that separates concerns and enables independent scaling of components.

## High-Level Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Interface Layer                    â”‚
â”‚                  (CLI / Future: Web Dashboard)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Orchestration Layer                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Agent Coordinator                           â”‚  â”‚
â”‚  â”‚  - Task Queue Management (p-queue)                   â”‚  â”‚
â”‚  â”‚  - Workflow Orchestration                            â”‚  â”‚
â”‚  â”‚  - Error Handling & Retry Logic                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Agent Layer                             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PaperReader   â”‚  â”‚ Entity        â”‚  â”‚ Relationship   â”‚ â”‚
â”‚  â”‚ Agent         â”‚â†’â”‚ Extractor     â”‚â†’â”‚ Mapper         â”‚ â”‚
â”‚  â”‚               â”‚  â”‚ Agent         â”‚  â”‚ Agent          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â†“                â†“                    â†“           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Validator Agent                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Service Layer                             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ LLM Service  â”‚  â”‚ Ingestion    â”‚  â”‚ Database        â”‚  â”‚
â”‚  â”‚ - OpenAI     â”‚  â”‚ - Semantic   â”‚  â”‚ - Supabase      â”‚  â”‚
â”‚  â”‚ - Anthropic  â”‚  â”‚   Scholar    â”‚  â”‚ - Queries       â”‚  â”‚
â”‚  â”‚              â”‚  â”‚ - arXiv      â”‚  â”‚ - Migrations    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ - PDF Parser â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Layer                                â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Supabase (PostgreSQL)                          â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  Node Tables: papers, concepts, methods, datasets     â”‚ â”‚
â”‚  â”‚  Edge Tables: citations, improvements, usage          â”‚ â”‚
â”‚  â”‚  Indexes: Full-text search, relationship lookups     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Details

### 1. Agent Layer

#### 1.1 Base Agent Class
- **Purpose**: Provides common functionality for all agents
- **Responsibilities**:
  - Task lifecycle management
  - Error handling and logging
  - Database access
  - Input validation
- **Pattern**: Template Method Pattern

#### 1.2 PaperReaderAgent
- **Purpose**: Fetch and parse research papers
- **Algorithm**: BFS traversal of citation network
- **Data Sources**:
  - Semantic Scholar API (metadata, citations)
  - arXiv API (PDF downloads)
- **Output**: Papers stored in database with full text

**BFS Traversal Algorithm:**
```
1. Initialize queue with seed paper
2. While queue not empty AND papers < limit:
   a. Dequeue paper ID
   b. Fetch paper from Semantic Scholar
   c. Store paper in database
   d. Fetch references and citations
   e. Sort by citation count
   f. Add top N to queue
3. Return collected papers
```

#### 1.3 EntityExtractorAgent
- **Purpose**: Extract entities using LLM
- **Strategy**: Multi-pass extraction
  - Pass 1: LLM extraction with structured prompts
  - Pass 2: Disambiguation (check for duplicates)
  - Pass 3: Validation and confidence scoring
- **Entities Extracted**:
  - Concepts (techniques, architectures)
  - Methods (algorithms, approaches)
  - Datasets (evaluation datasets)
  - Metrics (PSNR, SSIM, FPS)

#### 1.4 RelationshipMapperAgent
- **Purpose**: Map semantic relationships between entities
- **Relationships**:
  - Concept improvements (A improves B)
  - Citations (paper A cites paper B)
  - Method usage
  - Dataset evaluation
- **Classification**: LLM-based relationship classification

#### 1.5 ValidatorAgent
- **Purpose**: Ensure data quality and consistency
- **Validations**:
  - Duplicate detection
  - Graph consistency checks
  - Circular dependency detection
  - Confidence score validation

### 2. Orchestration Layer

#### Agent Coordinator
- **Concurrency**: Managed via p-queue (configurable limit)
- **Pipeline**:
```
  Reader â†’ Extractor â†’ Mapper â†’ Validator
```
- **Error Handling**: Promise.allSettled for batch processing
- **Features**:
  - Task queuing
  - Priority management
  - Timeout handling
  - Retry logic

### 3. Service Layer

#### 3.1 LLM Service
- **Multi-provider**: Supports OpenAI and Anthropic
- **Features**:
  - Structured JSON extraction
  - Retry with exponential backoff
  - Token usage tracking
  - Temperature control (default: 0.2 for consistency)
- **Prompts**: Template-based with domain expertise

#### 3.2 Ingestion Services
- **Semantic Scholar Client**:
  - Rate limiting: 600ms between requests
  - Batch operations
  - Citation network traversal
  
- **arXiv Client**:
  - PDF downloads with caching
  - Local storage management
  - URL construction
  
- **PDF Parser**:
  - Text extraction (pdf-parse)
  - Section detection
  - Reference parsing
  - Metadata extraction

#### 3.3 Database Service
- **Client**: Supabase (PostgreSQL)
- **Features**:
  - Connection pooling
  - Typed queries
  - Transaction support
  - Statistics tracking

### 4. Data Layer

#### 4.1 Schema Design

**Node Tables:**
- `papers`: Research papers with metadata
- `concepts`: Technical concepts and methods
- `methods`: Computational approaches
- `datasets`: Evaluation datasets
- `authors`: Paper authors

**Edge Tables:**
- `paper_cites`: Citation network
- `concept_improves_concept`: Improvement relationships
- `paper_introduces_concept`: Entity authorship
- `paper_uses_method`: Method usage
- `paper_evaluates_on_dataset`: Dataset evaluation

**Indexes:**
- Full-text search (GIN indexes)
- Relationship lookups (B-tree indexes)
- Date-based queries
- Normalized name lookups

#### 4.2 Graph Query Patterns

**Pattern 1: Traversal**
```sql
-- Find improvement chains
WITH RECURSIVE improvements AS (
  SELECT new_concept, old_concept, 1 as depth
  FROM concept_improves_concept
  WHERE old_concept = 'seed_concept_id'
  
  UNION ALL
  
  SELECT c.new_concept, c.old_concept, i.depth + 1
  FROM concept_improves_concept c
  JOIN improvements i ON c.old_concept = i.new_concept
  WHERE i.depth < 5
)
SELECT * FROM improvements;
```

**Pattern 2: Aggregation**
```sql
-- Most influential papers
SELECT p.title, COUNT(pc.source_paper) as citation_count
FROM papers p
LEFT JOIN paper_cites pc ON p.id = pc.target_paper
GROUP BY p.id
ORDER BY citation_count DESC;
```

## Data Flow

### Paper Processing Pipeline
```
1. Fetch Paper
   â””â”€> Semantic Scholar API
       â””â”€> Extract: title, abstract, authors, citations
           â””â”€> Store in papers table

2. Download PDF
   â””â”€> arXiv API
       â””â”€> Parse PDF
           â””â”€> Extract: full text, sections, references
               â””â”€> Update papers.pdf_text

3. Extract Entities
   â””â”€> LLM (OpenAI/Anthropic)
       â””â”€> Structured JSON extraction
           â””â”€> Disambiguate with existing entities
               â””â”€> Store: concepts, methods, datasets
                   â””â”€> Create: paper_introduces_concept links

4. Map Relationships
   â””â”€> For each concept pair:
       â””â”€> LLM classification
           â””â”€> Extract improvement type & gains
               â””â”€> Store: concept_improves_concept

5. Validate
   â””â”€> Check duplicates
       â””â”€> Verify consistency
           â””â”€> Detect circular dependencies
               â””â”€> Report validation results
```

## Scalability Considerations

### Current Limits
- **Papers**: 50-100 per run (configurable)
- **Concurrency**: 3 concurrent agents (configurable)
- **LLM Rate Limits**: OpenAI/Anthropic API limits
- **Database**: Supabase free tier (500MB)

### Scaling Strategies

**Horizontal Scaling:**
- Multiple agent workers processing different papers
- Distributed task queue (Redis/RabbitMQ)
- Load balancing across LLM providers

**Vertical Scaling:**
- Increase concurrency limits
- Larger database tier
- Caching layer (Redis) for frequent queries

**Optimization:**
- Batch LLM requests
- Parallel entity extraction
- Incremental updates (process only new papers)
- Vector embeddings for similarity search

## Error Handling

### Strategy
- **Retry Logic**: 3 attempts with exponential backoff
- **Graceful Degradation**: Continue processing other papers on failure
- **Logging**: Winston logger with multiple transports
- **Error Types**:
  - API errors (rate limits, timeouts)
  - Database errors (connection, constraints)
  - Parsing errors (PDF, JSON)
  - Validation errors (consistency checks)

### Recovery
- Failed papers logged for manual review
- Partial results saved (incremental progress)
- Task queue allows reprocessing specific papers

## Security

### API Keys
- Environment variables (never committed)
- Service role keys for privileged operations
- Rate limiting to prevent abuse

### Database
- Row-level security (RLS) in Supabase
- Parameterized queries (SQL injection prevention)
- Connection pooling

## Monitoring

### Metrics
- Papers processed per run
- Entity extraction success rate
- Relationship mapping accuracy
- Database query performance
- LLM token usage

### Logging
- Structured JSON logs
- Multiple log levels (debug, info, warn, error)
- Separate error log file
- Agent-specific context

## Technology Choices

### TypeScript
- Type safety for large codebase
- Better IDE support
- Compile-time error detection

### Supabase (PostgreSQL)
- Powerful graph queries
- ACID transactions
- Full-text search
- Managed hosting

### Multi-LLM Support
- Flexibility in provider choice
- Cost optimization
- Failover capability

### p-queue
- Concurrency control
- Priority queuing
- Timeout management

## Future Architecture Enhancements

### Phase 2: Vector Search
```
Add embedding layer:
- Generate embeddings for papers/concepts
- Store in pgvector extension
- Enable semantic similarity search
```

### Phase 3: Real-time Updates
```
Implement event-driven architecture:
- arXiv RSS feed monitoring
- Incremental updates
- Change detection
- Delta processing
```

### Phase 4: Microservices
```
Split into independent services:
- Paper Ingestion Service
- Entity Extraction Service
- Relationship Mapping Service
- Query Service (GraphQL API)
```

---

---

## ğŸ—ï¸ System Design Thinking & Real-World Scalability

### Design Principles

This system was architected with three core principles:

#### 1. **Separation of Concerns**
Each agent has a single, well-defined responsibility:
- **PaperReaderAgent:** Data acquisition only
- **EntityExtractorAgent:** Extraction and data quality only
- **RelationshipMapperAgent:** Graph construction only
- **ValidatorAgent:** Quality assurance only

**Benefit:** Easy to test, debug, and scale each component independently.

#### 2. **Fail-Safe Architecture**
- **Retry Logic:** All external API calls retry 3x with exponential backoff
- **Transaction Safety:** Database writes use transactions (all-or-nothing)
- **Error Isolation:** One paper failure doesn't crash the entire batch
- **Logging:** Comprehensive logs at every step for debugging

**Benefit:** System degrades gracefully under failure.

#### 3. **Data Quality First**
- **Multi-pass extraction:** 3 passes ensure high-quality entities
- **Confidence scoring:** Every extraction has 0-1 confidence score
- **Deduplication:** Prevents duplicate concepts across papers
- **Validation agent:** Separate quality control layer

**Benefit:** Production-ready data quality from day one.

---

### Scalability Considerations

#### Horizontal Scaling Strategy

**Current (POC):**
```
Single Process â†’ 5 papers in 15 minutes
```

**Phase 1 (Production):**
```
Load Balancer
     â†“
10 Worker Nodes â†’ 100 papers in 15 minutes
     â†“
Message Queue (SQS)
     â†“
Shared Database (Supabase)
```

**How it scales:**
- Add more workers â†’ Process more papers in parallel
- Message queue prevents duplicate work
- Stateless workers â†’ Easy to add/remove

#### Vertical Optimization

**Current bottlenecks:**
1. **LLM API calls:** 10 sec per paper
2. **PDF parsing:** 5 sec per paper
3. **Database writes:** 2 sec per paper

**Optimizations:**
1. **Batch LLM calls:** Process 5 papers in one call (3x faster)
2. **Async PDF parsing:** Parse while downloading next PDF
3. **Batch DB inserts:** Insert 100 entities at once (10x faster)

**Result:** 3 min/paper â†’ 30 sec/paper (6x improvement)

---

### Reliability Patterns

#### 1. Circuit Breaker Pattern
```typescript
// If OpenAI API fails 5 times in a row, switch to Anthropic
class LLMCircuitBreaker {
  failures = 0;
  threshold = 5;
  
  async call() {
    if (this.failures > this.threshold) {
      return this.fallbackProvider(); // Switch to Anthropic
    }
    
    try {
      const result = await this.openai.complete();
      this.failures = 0; // Reset on success
      return result;
    } catch (error) {
      this.failures++;
      throw error;
    }
  }
}
```

#### 2. Idempotency
```typescript
// Processing same paper twice produces same result
async processPaper(arxivId: string) {
  // Check if already processed
  const existing = await getPaperByArxivId(arxivId);
  if (existing) return existing; // Safe to re-run
  
  // Process new paper
  return await insertPaper(...);
}
```

#### 3. Graceful Degradation
```
If OpenAI API is down:
  â†’ Fall back to Anthropic Claude
  
If Semantic Scholar is down:
  â†’ Use arXiv API only
  
If database is slow:
  â†’ Queue writes, process in batch later
```

---

### Monitoring & Observability
```
Metrics to Track:
- Papers processed per hour
- Average extraction confidence
- LLM API latency (p50, p95, p99)
- Database query time
- Error rate by component
- Cost per paper processed

Alerts:
- Error rate > 5% â†’ Page on-call engineer
- Queue depth > 1000 â†’ Auto-scale workers
- LLM latency > 30s â†’ Investigate performance
```

---

### Security Considerations

**Current (POC):**
- API keys in .env file
- No authentication on database
- No rate limiting

**Production Requirements:**
1. **Secret Management:** AWS Secrets Manager / HashiCorp Vault
2. **Database Security:** Row-level security, encrypted connections
3. **API Rate Limiting:** Prevent abuse (100 requests/hour per user)
4. **Input Validation:** Sanitize all user inputs
5. **Audit Logging:** Track all data modifications

---

### Cost-Performance Tradeoffs

**Decision Matrix:**

| Component | Cheap Option | Expensive Option | Chosen | Why |
|-----------|-------------|------------------|---------|-----|
| LLM | GPT-3.5-turbo | GPT-4-turbo | GPT-4 | Quality critical for POC |
| Database | SQLite | Supabase Pro | Supabase Free | Fast setup, can upgrade |
| Compute | Single server | Kubernetes cluster | Single server | Adequate for POC |
| Storage | Local disk | S3 | Local disk | Low volume currently |

**Future adjustments:**
- Switch to GPT-3.5 for simple extractions (5x cheaper)
- Use S3 for PDF storage (scale to millions of papers)
- Add Redis cache (reduce DB load by 80%)

---

This architecture supports the current requirements while maintaining flexibility for future enhancements. The modular design allows each component to be scaled, replaced, or enhanced independently.  This architecture demonstrates **production-level thinking** even in a POC phase.

